## Solr properties
solr.httpbaseurl = http://${env:SOLR_SERVICE_HOST:-localhost}:${env:SOLR_SERVICE_PORT:-8983}/solr
solr.solrbaseurl = solr://${env:SOLR_SERVICE_HOST:-localhost}:${env:SOLR_SERVICE_PORT:-8983}/solr
solr.collection=gettingstarted

## solr alias handling (default solr.alias=false)
solr.alias=true
solr.collection.dateformat = yyyyMMdd_HHmm
solr.collection.create.params = numShards=1&replicationFactor=1&collection.configName=_default
solr.collection.keep = 2

## Other solr params
solr.uniquekey=id
## solr commit handling (default solr.commit=true)
solr.commit=true
## solr spellcheck rebuild
solr.spellcheck.dictionaries=nl,fr
solr.spellcheck.path=/spell
solr.spellcheck.query.nl=spellcheck.build=true&spellcheck.dictionary=file_nl
solr.spellcheck.query.fr=spellcheck.build=true&spellcheck.dictionary=file_fr
solr.spellcheck.delay=100

## camel routes setup
##importtask.file.uri=file:../target/data?fileName=data-import-gettingstarted.properties&idempotent=false&noop=true&delay=300s
importtask.file.uri=zookeeper://${env:ZOO_SERVICE_HOST:-localhost}:${env:ZOO_SERVICE_PORT:-32181}/data-import-gettingstarted?create=true&createMode=PERSISTENT&repeat=true
deltaimport.autostart=true
deltaimport.from.uri=timer://delta?delay=500&period=10000
## in case of file instead of zookeeper for {{importtask.file.uri}}: monitorzkclient.from.uri=direct:zkClientCheck
monitorzkclient.from.uri=timer://zkClientCheck?delay=60000&period=600000
monitorzkclient.log.level = INFO
threadpool.poolsize=200
aggregator.completionSize=20
aggregator.completionInterval=500000
aggregator.deletedDoc.completionSize = 1000
aggregator.deletedDoc.completionTimeout = 200

## sql properties
sql.pagesize=10
sql.latest=SELECT MAX(LAST_MODIFIED) FROM MASTER
##sql.idscount=SELECT COUNT(*) FROM MASTER WHERE LAST_MODIFIED>:?TimeStampFrom AND LAST_MODIFIED<=:?TimeStampTo
## mysql syntax - with pagesize
##sql.ids=SELECT ID_FIELD AS PROCESSID FROM MASTER WHERE LAST_MODIFIED>:?TimeStampFrom AND LAST_MODIFIED<=:?TimeStampTo ORDER BY ID_FIELD LIMIT :?OffSet, :?PageSize
## derby syntax - with pagesize
sql.ids=SELECT ID_FIELD AS PROCESSID FROM MASTER WHERE LAST_MODIFIED>:?TimeStampFrom AND LAST_MODIFIED<=:?TimeStampTo ORDER BY ID_FIELD OFFSET :?OffSet0 ROWS FETCH NEXT :?PageSize ROWS ONLY
## derby syntax - without pagesize
##sql.ids=SELECT ID_FIELD AS PROCESSID FROM MASTER WHERE LAST_MODIFIED>:?TimeStampFrom AND LAST_MODIFIED<=:?TimeStampTo ORDER BY ID_FIELD
##sql.ids=SELECT ID_FIELD AS PROCESSID FROM MASTER WHERE LAST_MODIFIED>:?TimeStampFrom AND LAST_MODIFIED<=:?TimeStampTo

sql.data.tables=master1,master2,master3
sql.data.master1=SELECT ID_FIELD, DATA AS IMPORT, LAST_MODIFIED, DATA, ROW_NUMBER() OVER() AS ROWNUM, CASE WHEN DATA='123' THEN TRUE ELSE FALSE END AS DELETECODE FROM MASTER WHERE ID_FIELD=:?ProcessId
sql.data.master2=SELECT * FROM MASTER2 WHERE ID_FIELD=:?ProcessId
sql.data.master3=SELECT * FROM MASTER3 WHERE ID_FIELD=:?ProcessId

## mapper properties
##mapper.tablename.fields contains csv list of fields to be imported with a possible mapping from db fieldname to Solr fieldname via notation <solr-fieldname>:<db-fieldname>
mapper.master1.fields=id:ID_FIELD,PRODUCT_ID:ROWNUM,CONTRIBUTORS:DATA,DELETIONCODE:DELETECODE,import_s:IMPORT,date_d:LAST_MODIFIED
mapper.field.deleteflag=DELETECODE
