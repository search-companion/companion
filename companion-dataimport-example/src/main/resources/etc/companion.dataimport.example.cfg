## Solr properties
solr.baseurl = ${env:SOLR_SERVICE_HOST:-localhost}:${env:SOLR_SERVICE_PORT:-8983}/solr
solr.httpbaseurl = http://${solr.baseurl}
solr.solrbaseurl = solr://${solr.baseurl}
solr.collection=gettingstarted

## solr alias handling (default solr.alias=false)
solr.alias=true
solr.collection.dateformat = yyyyMMdd_HHmm
solr.collection.create.params = numShards=1&replicationFactor=1&collection.configName=_default
solr.collection.keep = 2

## Other solr params
solr.uniquekey=id
## solr commit handling (default solr.commit=true)
solr.commit=true
## solr spellcheck rebuild
solr.spellcheck.dictionaries=
solr.spellcheck.path=/spell
solr.spellcheck.query.nl=spellcheck.build=true&spellcheck.dictionary=file_nl
solr.spellcheck.query.fr=spellcheck.build=true&spellcheck.dictionary=file_fr
solr.spellcheck.delay=100

## camel routes setup
importtask.file.uri=file:../target/data?fileName=data-import-gettingstarted.properties&idempotent=false&noop=true&delay=5s
##importtask.file.uri=zookeeper://${env:ZOO_SERVICE_HOST:-localhost}:${env:ZOO_SERVICE_PORT:-2181}/data-import-gettingstarted?create=true&createMode=PERSISTENT&repeat=true
deltaimport.autostart=true
deltaimport.from.uri=timer://delta?delay=10s&period=20s
## in case of file instead of zookeeper for {{importtask.file.uri}}: monitorzkclient.from.uri=direct:zkClientCheck-disabled
##monitorzkclient.from.uri=timer://zkClientCheck?delay=60000&period=600000
monitorzkclient.from.uri=direct:zkClientCheck-disabled
monitorzkclient.log.level = INFO
threadpool.poolsize=200
aggregator.completionSize=20
aggregator.completionInterval=500000
aggregator.deletedDoc.completionSize = 1000
aggregator.deletedDoc.completionTimeout = 200

## sql properties
sql.pagesize=10
sql.latest=SELECT MAX(last_modified) FROM items
##sql.idscount=SELECT COUNT(*) FROM items WHERE last_modified>:?TimeStampFrom AND last_modified<=:?TimeStampTo
sql.idscount=SELECT COUNT(*)::int FROM items WHERE last_modified>:?TimeStampFrom AND last_modified<=:?TimeStampTo
## mysql syntax - with pagesize
##sql.ids=SELECT ID_FIELD AS PROCESSID FROM MASTER WHERE LAST_MODIFIED>:?TimeStampFrom AND LAST_MODIFIED<=:?TimeStampTo ORDER BY ID_FIELD LIMIT :?OffSet, :?PageSize
## derby syntax - with pagesize
##sql.ids=SELECT ID_FIELD AS PROCESSID FROM MASTER WHERE LAST_MODIFIED>:?TimeStampFrom AND LAST_MODIFIED<=:?TimeStampTo ORDER BY ID_FIELD OFFSET :?OffSet0 ROWS FETCH NEXT :?PageSize ROWS ONLY
## postgresql syntax - with pagesize
sql.ids=SELECT item_id AS PROCESSID FROM items WHERE last_modified>:?TimeStampFrom AND last_modified<=:?TimeStampTo ORDER BY item_id LIMIT :?PageSize OFFSET :?OffSet0
## derby syntax - without pagesize
##sql.ids=SELECT ID_FIELD AS PROCESSID FROM MASTER WHERE LAST_MODIFIED>:?TimeStampFrom AND LAST_MODIFIED<=:?TimeStampTo ORDER BY ID_FIELD
##sql.ids=SELECT ID_FIELD AS PROCESSID FROM MASTER WHERE LAST_MODIFIED>:?TimeStampFrom AND LAST_MODIFIED<=:?TimeStampTo

sql.data.tables=items,items_language
sql.data.items=SELECT item_id, last_modified, supplier, status, CASE WHEN lower(STATUS)='inactive' THEN TRUE ELSE FALSE END AS DELETECODE FROM items WHERE item_id=:?ProcessId
sql.data.items_language=SELECT * FROM items_language WHERE item_id=:?ProcessId

## mapper properties
##mapper.tablename.fields contains csv list of fields to be imported with a possible mapping from db fieldname to Solr fieldname via notation <solr-fieldname>:<db-fieldname>
mapper.items.fields=id:item_id,date_dt:last_modified,deleted_b:DELETECODE,supplier_s:supplier,status_s:status
mapper.items_language.fields=language_id_ss:language_id,description_ss:description,features_ss:features
mapper.field.deleteflag=DELETECODE
